{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
        "\n",
        "**Due: Tuesday, January 21 @ 11:59 PM PST on Gradescope.**\n",
        "\n",
        "Assignment instructions:\n",
        "- Make sure each of the subquestions have answers\n",
        "- Ensure that group members indicate which problems they're in charge of\n",
        "- Show work and walk through your thought process where applicable\n",
        "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
        "- Document code with light comments (i.e. 'this function handles visualization')\n",
        "\n",
        "Submission instructions:\n",
        "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
        "\n",
        "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
        "\n",
        "https://github.com/imrisek-viktor/CME-241/Imrisek_assignment1.ipynb\n",
        "\n",
        "*Group members (replace below names with people in your group):* \n",
        "- Viktor Imrisek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "import graphviz\n",
        "from abc import ABC, abstractmethod\n",
        "from collections import defaultdict\n",
        "from dataclasses import dataclass\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "from pprint import pprint\n",
        "from typing import (Callable, Dict, Iterable, Generic, Sequence, Tuple,\n",
        "                    Mapping, TypeVar, Set)\n",
        "\n",
        "from rl.distribution import (Categorical, Distribution, FiniteDistribution,\n",
        "                             SampledDistribution)\n",
        "from rl.markov_process import FiniteMarkovProcess, MarkovProcess, NonTerminal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 1: Snakes and Ladders (Led by Viktor Imrisek)\n",
        "\n",
        "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
        "\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "How can we model this problem with a Markov Process?\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): MDP Modeling\n",
        "\n",
        "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Transition Probabilities\n",
        "\n",
        "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Modeling the Game\n",
        "\n",
        "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image below for the locations of the snakes and ladders.\n",
        "\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Snakes and ladders](figures\\snakesAndLadders.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (A) Answer\n",
        "The state space $\\mathcal{S} = \\{0, 1,2,\\ldots, 100\\}$ is a set of squares from $0$ to $100$, where $0$ is the starting position left of square $1$. The terminal state $\\mathcal{T}$ is the square $100$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (B) Answer\n",
        "By throwing a die we can get to $6$ different squares with equal probability. The only exception is when we are close to $100$, so when we overshoot we end up on $100$. Notice that 5 out of 6 transition probabilities from a square $n+1$ are the same as for square $n$. The transition probabilities $\\mathcal{P}: \\mathcal{N} \\cdot \\mathcal{S} \\rightarrow [0, 1]$ are:\n",
        "\n",
        "$$\\mathcal{P}(s_0, s_{38}) = \\frac{1}{6}, \\mathcal{P}(s_0, s_2) = \\frac{1}{6}, \\mathcal{P}(s_0, s_3) = \\frac{1}{6}, \\mathcal{P}(s_0, s_{14}) = \\frac{1}{6}, \\mathcal{P}(s_0, s_5) = \\frac{1}{6}, \\mathcal{P}(s_0, s_6) = \\frac{1}{6}$$\n",
        "\n",
        "$$\\mathcal{P}(s_1, s_2) = \\frac{1}{6}, \\mathcal{P}(s_1, s_3) = \\frac{1}{6}, \\mathcal{P}(s_1, s_{14}) = \\frac{1}{6}, \\mathcal{P}(s_1, s_5) = \\frac{1}{6}, \\mathcal{P}(s_1, s_6) = \\frac{1}{6}, \\mathcal{P}(s_1, s_7) = \\frac{1}{6}$$\n",
        "\n",
        "$$\\mathcal{P}(s_2, s_3) = \\frac{1}{6}, \\mathcal{P}(s_2, s_{14}) = \\frac{1}{6}, \\mathcal{P}(s_2, s_5) = \\frac{1}{6}, \\mathcal{P}(s_2, s_6) = \\frac{1}{6}, \\mathcal{P}(s_2, s_7) = \\frac{1}{6}, \\mathcal{P}(s_2, s_10) = \\frac{1}{6}$$\n",
        "$$ \\ldots $$\n",
        "\n",
        "An easier thing to do, is to send $s$ to $s+i$ for $i \\in [1,2,3,4,5,6]$ with equal probabilities. Then what we can do is to edit this for beginings of snakes and ladders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (C) Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "transition_map = {s : Categorical({s + 1:1/6, s + 2 : 1/6, s + 3:1/6, s + 4 : 1/6, s + 5:1/6, s + 6 : 1/6 }) for s in range(0,100)}\n",
        "# encode the ladders\n",
        "transition_map[1] = Categorical({38 : 1})\n",
        "transition_map[4] = Categorical({14 : 1})\n",
        "transition_map[8] = Categorical({10 : 1})\n",
        "transition_map[21] = Categorical({42 : 1})\n",
        "transition_map[28] = Categorical({76 : 1})\n",
        "transition_map[50] = Categorical({67 : 1})\n",
        "transition_map[71] = Categorical({92 : 1})\n",
        "transition_map[88] = Categorical({99 : 1})\n",
        "# encode the snakes\n",
        "transition_map[32] = Categorical({10 : 1})\n",
        "transition_map[36] = Categorical({6 : 1})\n",
        "transition_map[48] = Categorical({26 : 1})\n",
        "transition_map[63] = Categorical({18 : 1})\n",
        "transition_map[88] = Categorical({24 : 1})\n",
        "transition_map[95] = Categorical({56 : 1})\n",
        "transition_map[97] = Categorical({78 : 1})\n",
        "# fix the overshooting of a 100\n",
        "transition_map[95] = Categorical({100 : 5/6, 99 : 1/6, 98 : 1/6, 97 : 1/6, 96 : 1/6})\n",
        "transition_map[96] = Categorical({100 : 5/6, 99 : 1/6, 98 : 1/6, 97 : 1/6})\n",
        "transition_map[97] = Categorical({100 : 4/6, 99 : 1/6, 98 : 1/6})\n",
        "transition_map[98] = Categorical({100 : 5/6, 99 : 1/6})\n",
        "transition_map[99] = Categorical({100 : 1})\n",
        "\n",
        "markovSnake = FiniteMarkovProcess(transition_map)\n",
        "\n",
        "specialSquares = [1, 4, 8, 21, 28, 50, 71, 88, 32, 36, 48, 63, 88, 95, 97]\n",
        "trajectories = markovSnake.traces(Categorical({NonTerminal(0) : 1}))\n",
        "i = 0\n",
        "moveDist = defaultdict(float)\n",
        "numSamples = 1000\n",
        "for trajectory in trajectories:\n",
        "\n",
        "    i += 1\n",
        "    numMoves = 0\n",
        "    for st in trajectory:\n",
        "        # count if it is not a begining of a ladder or a snake as for those we teleport without adding time step\n",
        "        if st.state not in specialSquares:\n",
        "            numMoves += 1\n",
        "    moveDist[numMoves] += 1/numSamples\n",
        "    if i >= numSamples:\n",
        "        break\n",
        "# Extract keys and values from the dictionary to plot it\n",
        "keys = np.array(list(moveDist.keys()))  \n",
        "values = np.array(list(moveDist.values()))  \n",
        "sorted_indices = np.argsort(keys)\n",
        "keys = keys[sorted_indices]\n",
        "values = values[sorted_indices]\n",
        "plt.bar(keys, values)\n",
        "plt.title(\"Frequency Distribution of Numbers of Steps\")\n",
        "plt.xlabel(\"Number of Steps\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n",
        "    \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Question 2: Markov Decision Processes (Led by Viktor Imrisek)\n",
        "\n",
        "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
        "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
        "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): Optimal Value Function  \n",
        "\n",
        "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Optimal Policy  \n",
        "\n",
        "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Changing the Action Space  \n",
        "\n",
        "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
        "\n",
        "- The form of the Bellman optimality equation?\n",
        "- The optimal value function, $V^*(s)$?\n",
        "- The structure of the optimal policy, $\\pi^*(s)$?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (A) Answer\n",
        "\n",
        "$$ V^*(s) = \\max_{a \\in [0, 1]} \\Big\\{ a \\big[ (1 - a) + \\gamma V^*(s+1) \\big] + (1 - a) \\big[ (1 + a) + \\gamma V^*(s) \\big] \\Big\\} $$\n",
        "\n",
        "Since every state is symmetric, we will alwyas choose the same optimal action every time. Hence $V^*(s) = V^*(s+1)$ for all $s \\in \\mathcal{S}$. Thus,\n",
        "\n",
        "$$ V^*(s) = \\max_{a \\in [0, 1]} \\Big\\{ a \\big[ (1 - a) + \\gamma V^*(s) \\big] + (1 - a) \\big[ (1 + a) + \\gamma V^*(s) \\big] \\Big\\} $$\n",
        "$$ V^*(s) = \\max_{a \\in [0, 1]} \\Big\\{ a(1-a) + (1 - a)(1 + a) + \\gamma V^*(s) \\Big\\} $$\n",
        "$$ (1-\\gamma)V^*(s) = \\max_{a \\in [0, 1]} \\Big\\{ a(1-a) + (1 - a)(1 + a) \\Big\\} $$\n",
        "Substituting $\\gamma = \\frac{1}{2}$,\n",
        "$$ V^*(s) = 2 \\max_{a \\in [0, 1]} \\Big\\{ 1 + a - 2a^2 \\Big\\} $$\n",
        "\n",
        "Taking the derivative with respect to $a$ to obtain the value of $a$ that maximizes $V^*(s)$ we get\n",
        "$$1 - 4a^* = 0$$\n",
        "$$ a^* = \\frac{1}{4}$$\n",
        "\n",
        "Hence $V^*(s) = \\frac{9}{4}$.\n",
        "\n",
        "Another way to see this is this:\n",
        "$$G_t = \\sum_{i = t+1}^{\\infty} \\gamma^{i - t -1} R_i = R_{t+1} + \\gamma R_{t+2}  + \\gamma^2 R_{t+3}  + \\ldots $$\n",
        "\n",
        "Since we are taking the same action at each state, we have that $\\mathbb{E}(R_n) = \\mathbb{E}(R_m) = R^*$.\n",
        "\n",
        "Hence,\n",
        "\n",
        "$$\\mathbb{E}(G_t) = \\mathbb{E}(\\sum_{i = t+1}^{\\infty} \\gamma^{i - t -1} R_i )= \\sum_{i = t+1}^{\\infty} \\mathbb{E}(\\gamma^{i - t -1} R_i) =  \\sum_{i = t+1}^{\\infty} \\gamma^{i - t -1} R^* = 2R^*$$\n",
        "\n",
        "Now, let's calculate the the expected value of a reward $R_a$ given action $a$.\n",
        "\n",
        "$$\\mathbb{E}(R_a) = a \\cdot (1-a) + (1-a) \\cdot (1+a) = a - a^2 + 1 - a^2 = 1 + a - 2a^2$$\n",
        "\n",
        "Taking the derivative to obtain the value of $a$ leading to the highest expected $R_a$,\n",
        "\n",
        "$$1 - 4a^* = 0$$\n",
        "$$ a^* = \\frac{1}{4}$$\n",
        "\n",
        "From there we get $R^* = \\frac{9}{8}$.\n",
        "\n",
        "So, \n",
        "$$ V^* (s) = \\mathbb{E}(G_{t}^{*}) = \\frac{9}{4}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (B) Answer\n",
        "\n",
        "Since every state is symmetrical, we get that $\\pi^{*} (s) = a^* = \\frac{1}{4}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (C) Answer\n",
        "\n",
        "#### Bellman Optimality Equation Change and Optimal Value Function Change:\n",
        "$$ V^*(s) = \\max_{a \\in [0, \\frac{1}{s}]} \\Big\\{ a \\big[ (1 - a) + \\gamma V^*(s+1) \\big] + (1 - a) \\big[ (1 + a) + \\gamma V^*(s) \\big] \\Big\\} $$\n",
        "$$ V^*(s) = \\max_{a \\in [0, \\frac{1}{s}]} \\Big\\{ 1 + a - 2a^2 + a\\gamma V^*(s+1) + \\gamma V^*(s) - a \\gamma V^*(s) \\Big\\} $$\n",
        "taking the derivative with respect to $a$ to get a value of $a^*$:\n",
        "\n",
        "$$1 - 4a^* + \\gamma(V^*(s+1)-V^*(s))=0$$\n",
        "$$a^* = \\frac{1 + \\gamma(V^*(s+1)-V^*(s))}{4}$$\n",
        "\n",
        "#### Optimal Policy Change:\n",
        "Since $V^*(s+1)$ has more restricted action space than $V^*(s)$, we must have $V^*(s+1)-V^*(s)<0$. To choose optimal policy, we always compare the value of $a^*$ with $\\frac{1}{s}$ and choose the smaller one (this way we stay in the action space). Hence\n",
        "$$\\pi^* (s)=min\\{\\frac{1 + \\gamma(V^*(s+1)-V^*(s))}{4}, \\frac{1}{s}\\}$$\n",
        "\n",
        "Notice that if we set $V^*(s+1)=V^*(s)$, we get $a^* = \\frac{1}{4}$ as expected from parts a and b.\n",
        "Since $V^*(s+1)-V^*(s)$ will be generally small, we can guess that the policy will be close to to $min\\{\\frac{1}{4}, \\frac{1}{s}\\}$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 3: Frog in a Pond (Led by Viktor Imrisek)\n",
        "\n",
        "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
        "\n",
        "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
        "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
        "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
        "  \n",
        "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
        "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
        "\n",
        "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
        "\n",
        "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): MDP Modeling\n",
        "\n",
        "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components: \n",
        "\n",
        "- **State Space**: Define the possible states of the MDP. \n",
        "- **Action Space**: Specify the actions available to the frog at each state. \n",
        "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
        "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Python Implementation\n",
        "\n",
        "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
        "\n",
        "Write Python code that:\n",
        "\n",
        "- Models this MDP.\n",
        "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
        "\n",
        "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Visualization and Analysis\n",
        "\n",
        "After running the code, we observe the following graphs for $n=3$, $n=10$, and $n=25$:\n",
        "\n",
        "![FrogGraphs](figures/frogGraphs.png)\n",
        "\n",
        "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (A) Answer\n",
        "\n",
        "#### State Space:  \n",
        "The state space contains the information about which lilypad we are on. $\\mathcal{S} = \\{0, \\ldots, n \\}$.\n",
        "$0$ and $n$ are terminal, hence $\\mathcal{N}=\\{1,\\ldots, n-1 \\}$.\n",
        "\n",
        "\n",
        "#### Action Space:  \n",
        "\n",
        "Action sapce for each of the non-terminal states is $\\mathcal{A}(k) = \\{A,B\\}$ for $k \\in \\mathcal{N}$. \n",
        "\n",
        "#### Transition Function:  \n",
        "\n",
        "$$ P(s, A, s') = \\begin{cases}\n",
        "\\frac{i}{n}  & \\text{if} & s = i, s' = i-1 \\text{  for  } i \\in \\mathcal{N}\\\\\n",
        "\\frac{n-i}{n} & \\text{if} & s = i, s' = i + 1 \\text{  for  } i \\in \\mathcal{N}\n",
        "\\end{cases}$$\n",
        "\n",
        "$$P(s, B, s') = \\begin{cases}\n",
        "\\frac{i}{n} & \\text{if} & s=i, s'=k \\text{  for  } i \\neq k, i \\in \\mathcal{N}, k \\in \\mathcal{S}\n",
        "\\end{cases}$$\n",
        "\n",
        "#### Reward Function:  \n",
        "The only reward we get is if we escape.\n",
        "$$ R(s, a, s') = \\begin{cases}\n",
        "0  & \\text{if} & s' \\neq n \\\\\n",
        "1  & \\text{if} & s' = n\n",
        "\\end{cases}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (B) Answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "MDPRefined = dict\n",
        "def j_range (i,n): #I defined this to sepcify that action B cannot lead to the same state it started from\n",
        "    j_values = []\n",
        "    for j in range(i):\n",
        "        j_values.append(j)\n",
        "    for j in range(i+1, n+1):\n",
        "        j_values.append(j)\n",
        "    return j_values\n",
        "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
        "    \n",
        "    data = {\n",
        "        i: {\n",
        "            'A': {\n",
        "                i - 1: i/n, # TODO: fill in with the correct transition probabilities\n",
        "                i + 1: (n-i)/n, # TODO: fill in with the correct transition probabilities\n",
        "            },\n",
        "            'B': {\n",
        "                j: i/n for j in j_range(i,n) # TODO: fill in with the correct transition probabilities\n",
        "            }\n",
        "        } for i in range(1, n)\n",
        "    }\n",
        "    data[0] = {} # TODO: this is the initial state, so what would be the correct transition probabilities?\n",
        "    data[n] = {} # TODO: similarly, this is the terminal state, so what would be the correct transition probabilities?\n",
        "    # empty since they are both terminal\n",
        "\n",
        "    gamma = 1.0\n",
        "    return MDPRefined(data, gamma)\n",
        "\n",
        "Mapping = dict\n",
        "def direct_bellman(n: int) -> Mapping[int, float]:\n",
        "    vf = [0.5] * (n + 1)\n",
        "    vf[0] = 0.\n",
        "    vf[n] = 0.\n",
        "    tol = 1e-8\n",
        "    epsilon = tol * 1e4\n",
        "    while epsilon >= tol:\n",
        "        old_vf = [v for v in vf]\n",
        "        total = sum(old_vf)\n",
        "        for i in range(1, n):\n",
        "            if i + 1 == n:\n",
        "                reward_A = 1\n",
        "            else:\n",
        "                reward_A = 0\n",
        "            A_value = (i / n) * old_vf[i - 1] + ((n - i) / n) * (reward_A + old_vf[i + 1])\n",
        "            #separately compute actions A and B\n",
        "            B_value = (1 / n) * (1 + (total - old_vf[i]))\n",
        "\n",
        "            vf[i] = max(A_value, B_value) # TODO: fill in with the Bellman update\n",
        "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
        "    return {v: f for v, f in enumerate(vf)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (C) Answer\n",
        "\n",
        "The structure of the policy stays the same. For $i,j \\in [2,n-2]$ we observe $Q(i, A) \\approx Q(j,A), Q(i, B) \\approx Q(j , B)$ and furthermore $Q(i, A)> Q(j, B)$. For Lilypad $1$, we always have the opposite trend, namely $Q(1, A) < Q(1, B)$. For Lilipad $n-1$, we just have $Q(n-1, A)$ even bigger.\n",
        "\n",
        "Hence we choose action B if and only if we are at Lilipad $1$. Else we choose $A$. So for lilypad 13 we choose A and for lilypad 1 we choose B.\n",
        "\n",
        "The values of Q are roughly in the same ballpark (around $0.7$) for all graphed $n$. But the bigger the $n$ the smaller the difference between Q(A) and Q(B)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 4: Manual Value Iteration (Led by Viktor Imrisek)\n",
        "\n",
        "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} = \\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
        "$$\\mathcal{P}: \\mathcal{N} \\cdot \\mathcal{A} \\cdot \\mathcal{S} \\rightarrow [0, 1]$$  \n",
        "is defined as:  \n",
        "$$\\mathcal{P}(s_1, a_1, s_1) = 0.25, \\mathcal{P}(s_1, a_1, s_2) = 0.65, \\mathcal{P}(s_1, a_1, s_3) = 0.1$$  \n",
        "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.4, \\mathcal{P}(s_1, a_2, s_3) = 0.5$$  \n",
        "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.15, \\mathcal{P}(s_2, a_1, s_3) = 0.55$$  \n",
        "$$\\mathcal{P}(s_2, a_2, s_1) = 0.25, \\mathcal{P}(s_2, a_2, s_2) = 0.55, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$  \n",
        "\n",
        "The Reward Function  \n",
        "$$\\mathcal{R}: \\mathcal{N} \\cdot \\mathcal{A} \\rightarrow \\mathbb{R}$$  \n",
        "is defined as:  \n",
        "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$  \n",
        "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$  \n",
        "\n",
        "Assume a discount factor of $\\gamma = 1$.\n",
        "\n",
        "### Problem Statement\n",
        "\n",
        "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### Subquestions\n",
        "\n",
        "#### Part (A): 2 Iterations\n",
        "\n",
        "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (B): Argument\n",
        "\n",
        "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (C): Policy Evaluation\n",
        "\n",
        "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
        "\n",
        "---\n",
        "\n",
        "#### Part (D): Sensitivity Analysis\n",
        "\n",
        "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
        "\n",
        "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
        "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (A) Answer\n",
        "The value iteration update is given by:\n",
        "\n",
        "$$v_k(s) = \\max_{a \\in \\mathcal{A}} q_k(s, a)$$\n",
        "where:\n",
        "$$q_k(s, a) = \\mathcal{R}(s, a) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s, a, s') \\cdot v_{k-1}(s')$$\n",
        "Initialize:\n",
        "$$v_0(s_1) = 10.0, \\quad v_0(s_2) = 1.0, \\quad v_0(s_3) = 0.0$$\n",
        "\n",
        "First iteration:\n",
        "For $s_1, a_1$:\n",
        "$$q_1(s_1, a_1) = \\mathcal{R}(s_1, a_1) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s_1, a_1, s') \\cdot v_0(s')$$\n",
        "\n",
        "Substituting:\n",
        "$$q_1(s_1, a_1) = 8.0 + (0.25 \\cdot 10.0 + 0.65 \\cdot 1.0 + 0.1 \\cdot 0.0) = 8.0 + (2.5 + 0.65 + 0.0) = 11.15$$\n",
        "\n",
        "For $s_1, a_2$:\n",
        "\n",
        "$$q_1(s_1, a_2) = \\mathcal{R}(s_1, a_2) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s_1, a_2, s') \\cdot v_0(s')$$\n",
        "Substituting:\n",
        "$$q_1(s_1, a_2) = 10.0 + (0.1 \\cdot 10.0 + 0.4 \\cdot 1.0 + 0.5 \\cdot 0.0) = 10.0 + (1.0 + 0.4 + 0.0) = 11.4$$\n",
        "\n",
        "Thus:\n",
        "$$v_1(s_1) = \\max \\{q_1(s_1, a_1), q_1(s_1, a_2)\\} = \\max \\{11.15, 11.4\\} = 11.4$$\n",
        "$$\\pi_1(s_1) = \\arg\\max_{a \\in \\mathcal{A}} \\{q_1(s_1, a_1), q_1(s_1, a_2)\\} = a_2$$\n",
        "For $s_2, a_1$:\n",
        "$$q_1(s_2, a_1) = \\mathcal{R}(s_2, a_1) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s_2, a_1, s') \\cdot v_0(s')$$\n",
        "Substituting:\n",
        "$$q_1(s_2, a_1) = 1.0 + (0.3 \\cdot 10.0 + 0.15 \\cdot 1.0 + 0.55 \\cdot 0.0) = 1.0 + (3.0 + 0.15 + 0.0) = 4.15$$\n",
        "\n",
        "For $s_2, a_2$:\n",
        "$$q_1(s_2, a_2) = \\mathcal{R}(s_2, a_2) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s_2, a_2, s') \\cdot v_0(s')$$\n",
        "Substituting:\n",
        "$$q_1(s_2, a_2) = -1.0 + (0.25 \\cdot 10.0 + 0.55 \\cdot 1.0 + 0.2 \\cdot 0.0) = -1.0 + (2.5 + 0.55 + 0.0) = 2.05$$\n",
        "Thus:\n",
        "$$v_1(s_2) = \\max \\{q_1(s_2, a_1), q_1(s_2, a_2)\\} = \\max \\{4.15, 2.05\\} = 4.15$$\n",
        "$$\\pi_1(s_2) = \\arg\\max_{a \\in \\mathcal{A}} \\{q_1(s_2, a_1), q_1(s_2, a_2)\\} = a_1$$\n",
        "\n",
        "Finally, $v_1(s_3)=0$ since $s_3$ is terminal.\n",
        "\n",
        "Second iteration:\n",
        "$$q_2(s_1, a_1) = 8.0 + 1 \\cdot (0.25 \\cdot 11.4 + 0.65 \\cdot 4.15 + 0.1 \\cdot 0.0)= 13.55$$\n",
        "\n",
        "$$q_2(s_1, a_2) = 10.00 + 1 \\cdot (0.1 \\cdot 11.4 + 0.4 \\cdot 4.15 + 0.5 \\cdot 0.0)= 12.8$$\n",
        "\n",
        "$$v_2(s_1)= max(13.55, 12.8) = 13.55$$\n",
        "$$\\pi_2 (s_1) = a_1$$\n",
        "\n",
        "$$q_2(s_2, a_1) = 1.0 + 1 \\cdot (0.3 \\cdot 11.4 + 0.15 \\cdot 4.15 + 0.55 \\cdot 0.0)= 5.04$$\n",
        "\n",
        "$$q_2(s_3, a_2) = -1.0 + 1 \\cdot (0.25 \\cdot 11.4 + 0.55 \\cdot 4.15 + 0.2 \\cdot 0.0)= 4.13$$\n",
        "\n",
        "$$v_2(s_2)= max(5.04, 4.13) = 5.04$$\n",
        "$$\\pi_2 (s_2) = a_1$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (B) Answer:  \n",
        "\n",
        "Value iteration update is defined by\n",
        "$$v_{k}(s) = \\max_{a \\in \\mathcal{A}} \\; q_{k}(s,a), \\quad \\text{where} \\quad q_{k}(s,a) = \\mathcal{R}(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s,a,s')\\, v_{k-1}(s')$$\n",
        "The greedy policy at iteration $k$ is then given by\n",
        "$$\\pi_k(s) = \\arg\\max_{a \\in \\mathcal{A}} \\; q_{k}(s,a)$$\n",
        "\n",
        "In our MDP the rewards $\\mathcal{R}(s,a)$ and transition probabilities $\\mathcal{P}(s,a,s')$ are fixed, and there is an absorbing terminal state $s_3$. Consequently, the updates of $v_k(s)$ depend only on the previous value function $v_{k-1}(s)$. Because the state space is finite and the dynamics (including the terminal nature of $s_3$) ensure that the effect of future rewards is limited, the value function sequence $\\{v_k(s)\\}$ converges in a finite number of steps.\n",
        "\n",
        "Specifically, after two iterations we obtained $v_2(s)$ and the corresponding greedy policy $\\pi_2(s)$. By the structure of the Bellman update, using $v_2(s)$ to compute $q_3(s,a)$ yields the same maximizers as in the second iteration, that is,\n",
        "$$v_{3}(s) = \\max_{a \\in \\mathcal{A}} \\Bigl\\{ \\mathcal{R}(s,a) + \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}(s,a,s')\\, v_{2}(s') \\Bigr\\} = v_2(s)$$\n",
        "which in turn implies\n",
        "\n",
        "$$\\pi_3(s) = \\pi_2(s)$$\n",
        "\n",
        "Since the Bellman operator is monotonic and the state space is finite, the policy cannot change once the value function has converged. Therefore, for all $k > 2$ we have $\\pi_k(s) = \\pi_2(s)$, establishing that the policy $\\pi_2(\\cdot)$ is optimal.\n",
        "\n",
        "$$\\pi^*(s_1)=a_1 \\quad \\pi^*(s_2)=a_1$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (C) Answer:  \n",
        "\n",
        "Under the policy $\\pi_2$ we have\n",
        "$\n",
        "\\pi_2(s_1)=a_1 \\quad \\text{and} \\quad \\pi_2(s_2)=a_1,\n",
        "$\n",
        "and the terminal state $s_3$ satisfies $V^{\\pi_2}(s_3)=0$. Therefore, the Bellman equations for policy evaluation are:\n",
        "\n",
        "\n",
        "$$V^{\\pi_2}(s_1) = \\mathcal{R}(s_1,a_1) + \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}(s_1,a_1,s')\\, V^{\\pi_2}(s') = 8.0 + 0.25\\,V^{\\pi_2}(s_1) + 0.65\\,V^{\\pi_2}(s_2) + 0.1\\cdot 0 $$\n",
        "$$V^{\\pi_2}(s_2) = \\mathcal{R}(s_2,a_1) + \\sum_{s'\\in\\mathcal{S}} \\mathcal{P}(s_2,a_1,s')\\, V^{\\pi_2}(s') = 1.0 + 0.3\\,V^{\\pi_2}(s_1) + 0.15\\,V^{\\pi_2}(s_2) + 0.55\\cdot 0$$\n",
        "\n",
        "\n",
        "Let $x = V^{\\pi_2}(s_1)$ and $y = V^{\\pi_2}(s_2)$. Then the equations become:\n",
        "\n",
        "\n",
        "$$x = 8 + 0.25\\,x + 0.65\\,y$$\n",
        "$$y = 1 + 0.3\\,x + 0.15\\,y$$\n",
        "\n",
        "\n",
        "Rearrange the first equation:\n",
        "$$\n",
        "x - 0.25\\,x - 0.65\\,y = 8 \\quad\\Longrightarrow\\quad 0.75\\,x - 0.65\\,y = 8. \\tag{1}\n",
        "$$\n",
        "Rearrange the second equation:\n",
        "$$\n",
        "y - 0.15\\,y = 1 + 0.3\\,x \\quad\\Longrightarrow\\quad 0.85\\,y = 1 + 0.3\\,x \\quad\\Longrightarrow\\quad y = \\frac{1 + 0.3\\,x}{0.85}. \\tag{2}\n",
        "$$\n",
        "Substitute $(2)$ into $(1)$:\n",
        "$\n",
        "0.75\\,x - 0.65\\,\\left(\\frac{1+0.3\\,x}{0.85}\\right) = 8.\n",
        "$\n",
        "Multiplying both sides by $0.85$ to clear the denominator,\n",
        "$\n",
        "0.75\\,(0.85)x - 0.65\\,(1+0.3\\,x) = 8\\,(0.85).\n",
        "$\n",
        "That is,\n",
        "$$0.6375\\,x - 0.65 - 0.195\\,x = 6.8$$\n",
        "Combine like terms:\n",
        "$$(0.6375 - 0.195)x = 6.8 + 0.65 \\quad\\Longrightarrow\\quad 0.4425\\,x = 7.45$$\n",
        "Thus,\n",
        "$$x = \\frac{7.45}{0.4425} \\approx 16.84$$\n",
        "Now, substituting $x$ back into $(2)$:\n",
        "$$y = \\frac{1 + 0.3(16.84)}{0.85} = \\frac{1 + 5.052}{0.85} = \\frac{6.052}{0.85} \\approx 7.12$$\n",
        "\n",
        "Final Answer:\n",
        "$$V^{\\pi_2}(s_1) \\approx 16.84,\\quad V^{\\pi_2}(s_2) \\approx 7.12,\\quad V^{\\pi_2}(s_3) = 0$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part (D) Answer\n",
        "#### Value Iteration:  \n",
        "\n",
        "We initialize \n",
        "$$\n",
        "v_0(s_1)=10.0,\\quad v_0(s_2)=1.0,\\quad v_0(s_3)=0.\n",
        "$$\n",
        "For $ s_1 $:\n",
        "\n",
        "$$ q_1(s_1,a_1) = R(s_1,a_1) + \\Bigl(0.25\\cdot10.0 + 0.65\\cdot1.0 + 0.1\\cdot0\\Bigr) = 8.0 + (2.5 + 0.65) = 11.15 $$\n",
        "$$q_1(s_1,a_2) = R(s_1,a_2) + \\Bigl(0.1\\cdot10.0 + 0.4\\cdot1.0 + 0.5\\cdot0\\Bigr) = 11.0 + (1.0 + 0.4) = 12.4.$$\n",
        "Thus,\n",
        "$$\n",
        "v_1(s_1)=\\max\\{11.15,\\,12.4\\}=12.4,\\quad \\pi_1(s_1)=a_2.\n",
        "$$\n",
        "\n",
        "For $ s_2 $:\n",
        "\n",
        "$$q_1(s_2,a_1) = 1.0 + \\Bigl(0.3\\cdot10.0 + 0.15\\cdot1.0 + 0.55\\cdot0\\Bigr) = 1.0 + (3.0 + 0.15) = 4.15 $$\n",
        "$$q_1(s_2,a_2) = -1.0 + \\Bigl(0.25\\cdot10.0 + 0.55\\cdot1.0 + 0.2\\cdot0\\Bigr) = -1.0 + (2.5 + 0.55) = 2.05$$\n",
        "Thus,\n",
        "$$\n",
        "v_1(s_2)=4.15,\\quad \\pi_1(s_2)=a_1.\n",
        "$$\n",
        "\n",
        "For the terminal state $ s_3 $, $ v_1(s_3)=0 $.\n",
        "\n",
        "\n",
        "#### Optimal Deterministic Policy:  \n",
        "\n",
        "The modified reward increases $ q_1(s_1,a_2) $ from $ 11.4 $ to $ 12.4 $, but the greedy choice for $ s_1 $ was already $ a_2 $ (since $ 11.15 < 11.4 $ originally). Hence, the optimal action after one iteration at $ s_1 $ remains $ a_2 $, and the optimal action after one iteration at $ s_2 $ remains $ a_1 $.\n",
        "\n",
        "$$\n",
        "\\pi_{1}(s_1)=a_2,\\quad \\pi_{1}(s_2)=a_1,\\quad \\pi_{1}(s_3)=\\text{(terminal)}\n",
        "$$\n",
        "\n",
        "The change in $ R(s_1,a_2) $ will always add $1$ to the results we obtained previously for action $a_2$ in state $s_1$. We can see that the difference for second iteration was less than one, so here we would have a change compared to the previous results. Although, it does not affect the Optimal Deterministic Policy after the first iteration, it could lead to a different policy later."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Question 5: Fixed-Point and Policy Evaluation True/False Questions (Led by Viktor Imrisek)\n",
        "\n",
        "### Recall Section: Key Formulas and Definitions\n",
        "\n",
        "#### Bellman Optimality Equation\n",
        "The Bellman Optimality Equation for state-value functions is:\n",
        "$$\n",
        "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
        "$$\n",
        "For action-value functions:\n",
        "$$\n",
        "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
        "$$\n",
        "\n",
        "#### Contraction Property\n",
        "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
        "$$\n",
        "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
        "$$\n",
        "This guarantees convergence to a unique fixed point.\n",
        "\n",
        "#### Policy Iteration\n",
        "Policy Iteration alternates between:\n",
        "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
        "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
        "   $$\n",
        "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
        "   $$\n",
        "\n",
        "#### Discounted Return\n",
        "The discounted return from time step $t$ is:\n",
        "$$\n",
        "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
        "$$\n",
        "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
        "\n",
        "### True/False Questions (Provide Justification)\n",
        "\n",
        "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
        "\n",
        "---\n",
        "\n",
        "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
        "\n",
        "---\n",
        "\n",
        "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
        "\n",
        "---\n",
        "\n",
        "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
        "\n",
        "---\n",
        "\n",
        "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers (Provide justification, brief explanations are fine)\n",
        "\n",
        "#### Question 1:  \n",
        "\n",
        "True.\n",
        "\n",
        "Justification:\n",
        "Recall the definition of the action-value function for a policy $\\pi$:\n",
        "$$\n",
        "Q^\\pi(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s'),\n",
        "$$\n",
        "where $V^\\pi(s')$ is the state-value function under policy $\\pi$.\n",
        "\n",
        "If the immediate reward $R(s, a)$ increases by $2$, the updated reward is $R(s, a) + 2$. Therefore, the updated action-value function becomes:\n",
        "$$\n",
        "Q^\\pi_{\\text{new}}(s, a) = \\bigl(R(s, a) + 2\\bigr) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s').\n",
        "$$\n",
        "\n",
        "Since the term $\\gamma \\sum_{s'} P(s, a, s') V^\\pi(s')$ remains unchanged, we have:\n",
        "$$\n",
        "Q^\\pi_{\\text{new}}(s, a) = Q^\\pi(s, a) + 2.\n",
        "$$\n",
        "\n",
        "Thus, the updated $Q^\\pi(s, a)$ increases by $2$, and the statement is \\textbf{True}.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### Question 2:  \n",
        "\n",
        "True.\n",
        "\n",
        "Justification: Lets calculate the discounted return using the formula:\n",
        "$$G_0 = R_1 + \\gamma R_2 + \\gamma^2 R_3$$\n",
        "For $\\gamma = 0.9$ and the given rewards $R_1 = 5$, $R_2 = 3$, and $R_3 = 1$, we have:\n",
        "$$G_0 = 5 + 0.9 \\cdot 3 + (0.9)^2 \\cdot 1$$\n",
        "Calculating each term:\n",
        "$$5 + 0.9 \\cdot 3 = 5 + 2.7 = 7.7$$\n",
        "$$(0.9)^2 \\cdot 1 = 0.81$$\n",
        "\n",
        "So, the total discounted return is:\n",
        "$$G_0 = 7.7 + 0.81 = 8.51$$\n",
        "\n",
        "Since $8.51 > 6$, the statement is True.\n",
        "\n",
        "\n",
        "This makes sense because even with the discount factor $\\gamma = 0.9$, does not discount that much.\n",
        "\n",
        "#### Question 3:  \n",
        "True.\n",
        "\n",
        "Justification: Look at 6/36 slide from Tour - DP.\n",
        "\n",
        "The Bellman Policy Operator is defined as\n",
        "$$\n",
        "B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V\n",
        "$$\n",
        "and by Banach Fixed-point theorem it is a contraction mapping under the $L^\\infty$-norm provided that $\\gamma \\in [0, 1)$. \n",
        "\n",
        "This contraction property described in the problem statement implies that for any two value functions $X$ and $Y$, we have\n",
        "$$\n",
        "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty\n",
        "$$\n",
        "Since $\\gamma < 1$, the mapping will bring successive approximations closer together, guaranteeing convergence to a unique fixed point $V^\\pi$, which is the solution to\n",
        "$$\n",
        "V^\\pi = B^\\pi(V^\\pi)\n",
        "$$\n",
        "\n",
        "Thus, the statement is True. \n",
        "#### Question 4:  \n",
        "False\n",
        "\n",
        "Justification: In Policy Iteration, the Policy Improvement step uses the rule which is monotone:\n",
        "$$\n",
        "\\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right],\n",
        "$$\n",
        "which guarantees that the new policy $\\pi'$ is at least as good as the previous policy $\\pi$. This means that the value function satisfies:\n",
        "$$\n",
        "V^{\\pi'}(s) \\geq V^\\pi(s) \\quad \\text{for all } s.\n",
        "$$\n",
        "However, $\\pi'$ is not necessarily strictly better than $\\pi$ at every state unless there exists at least one state $s$ where the improvement is strict. Therefore, the policy improvement step does not guarantee that the updated policy performs strictly better in every iteration.\n",
        "\n",
        "Since the optimal policy exists (By the previous question it must converge), if we plug in the optimal policy, the update policy cannot be strictly better.\n",
        "\n",
        "Thus, the statement is False.\n",
        "\n",
        "#### Question 5:  \n",
        "\n",
        "True\n",
        "\n",
        "Justification: \n",
        "The state-value function is defined as\n",
        "$$\n",
        "V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s, a).\n",
        "$$\n",
        "Since $Q^\\pi(s, a)$ of for every action is $10$ and $\\sum_a \\pi(a|s) = 1$ , it follows that\n",
        "$$\n",
        "V^\\pi(s) = \\sum_a \\pi(a|s) \\cdot 10 = 10 \\sum_a \\pi(a|s) = 10.\n",
        "$$\n",
        "\n",
        "\n",
        "Therefore, the statement is True.\n",
        "\n",
        "#### Question 6:  \n",
        "\n",
        "True\n",
        "\n",
        "Justification: We are given the discounted return:\n",
        "$$\n",
        "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i.\n",
        "$$\n",
        "Assume that the rewards are bounded, so there exists a constant $M$ such that $|R_i| \\leq M$ for all $i$. Then we have:\n",
        "$$\n",
        "|G_t| \\leq \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} |R_i| \\leq M \\sum_{i=t+1}^\\infty \\gamma^{i-t-1}.\n",
        "$$\n",
        "Changing the index by letting $k = i-t-1$, the sum becomes:\n",
        "$$\n",
        "\\sum_{k=0}^\\infty \\gamma^k.\n",
        "$$\n",
        "Since $\\gamma < 1$, this geometric series converges to:\n",
        "$$\n",
        "\\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}.\n",
        "$$\n",
        "Thus,\n",
        "$$\n",
        "|G_t| \\leq \\frac{M}{1-\\gamma},\n",
        "$$\n",
        "which shows that $G_t$ is bounded and converges to a finite value.\n",
        "\n",
        "Therefore, the statement is True."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
